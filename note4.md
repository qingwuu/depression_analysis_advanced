# LDA 与 Count 向量 vs TF-IDF

## 1. 为什么 LDA 更偏好 Count 向量而不是 TF-IDF

### LDA 的数学假设

LDA（Latent Dirichlet Allocation，潜在狄利克雷分布分配）是一个**概率生成模型**。它假设文档的生成过程如下：

1. **文档级别**：

   - 每个文档先从 Dirichlet 分布中采样一个主题分布 θ。
   - 例如：文档 A 可能是 70% 抑郁相关 + 30% 治疗相关。

2. **单词级别**：

   - 每个单词先根据 θ 随机选择一个主题 z。
   - 再从该主题的词分布 φ 中采样出一个单词。

因此，LDA 的数学推导依赖于：

- **单词的出现次数（count）**，而不是加权特征。
- 假设每个单词的生成都来自一个**多项式分布（Multinomial distribution）**。

### 为什么不用 TF-IDF

- **TF-IDF 的本质**：它是一种人为加权方法，旨在降低常见词（如“the”、“and”）的权重，突出稀有但信息量大的词。
- **问题**：

  - TF-IDF 会把真实的“出现次数”转换成浮点数。例如某词实际出现 10 次，TF-IDF 可能变为 2.3。
  - LDA 若把 2.3 当成“单词出现 2.3 次”，在概率建模中没有物理意义。
  - LDA 的推导要求整数计数，才能与其基于 **Multinomial 分布** 的假设一致。

### Count vs TF-IDF 对比

- **CountVectorizer**：

  - 生成整数词频矩阵。
  - 与 LDA 的生成假设严格匹配。

- **TF-IDF**：

  - 生成浮点加权矩阵。
  - 在 sklearn 等实现中，LDA 仍然能运行，但其 **概率意义会失真**：

    - θ（文档 → 主题分布）
    - φ（主题 → 词分布）

  - 不再是真实的概率，只是近似的权重。

### 结论

- **LDA** 是生成模型，需要整数计数作为输入。
- **TF-IDF** 更适合判别模型（如分类、聚类），但不完全符合 LDA 的建模假设。

👉 **一句话总结**：LDA 要求输入是“单词出现次数（Count）”，而 TF-IDF 是加权特征，更适合判别式任务。

---

## 2. LDA 的结构是怎么得到的？怎么决定的？

### LDA 的层次化概率图模型

LDA 是一个 **分层贝叶斯模型（Hierarchical Bayesian Model）**，其结构源自对“文档由主题生成、主题由词生成”的假设。

#### 三层生成结构

1. **文档级别 (Document level)**：

   - 每篇文档有一个主题分布 θ。
   - θ \~ Dirichlet(α)。
   - 超参数 α 控制主题稀疏性：

     - α 小 → 文档集中于少数主题。
     - α 大 → 文档更均匀地分布在多个主题。

2. **主题级别 (Topic level)**：

   - 每个主题有一个词分布 φ。
   - φ \~ Dirichlet(β)。
   - 超参数 β 控制词稀疏性：

     - β 小 → 主题集中于少数关键词。
     - β 大 → 主题更均匀地包含很多词。

3. **单词级别 (Word level)**：

   - 每个单词：

     - 先根据 θ 选择一个主题 z。
     - 再根据 φ 从该主题中选择一个单词 w。

#### 为什么是这个结构？

- **概率图模型的思想**：我们希望解释“文档里为什么会出现这样的词”。
- **自然的推导**：假设文档由多个主题混合生成，而主题再决定用词。
- **数学便利性**：

  - Dirichlet 是 Multinomial 的共轭先验，方便推导和推断。
  - α、β 可以控制稀疏性，从而灵活建模不同语料。

### 小结

- **输入层**：单词计数矩阵（CountVectorizer）。
- **中间层**：

  - 文档的主题分布 θ。
  - 主题的词分布 φ。

- **超参数**：

  - α（文档 → 主题稀疏性）。
  - β（主题 → 词稀疏性）。
  - K（主题数）。

- **输出**：

  - 每个文档的主题分布。
  - 每个主题的关键词。

👉 LDA 的结构是 **统计建模 + 贝叶斯推断** 的结果，而不是随意设计出来的。
